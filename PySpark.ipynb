{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjNetOOjdjwe",
        "outputId": "94f6225f-8386-4af4-d41e-9270eb6be60c"
      },
      "id": "ZjNetOOjdjwe",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=6a97eb20052ae8ecfe6209e1528bb9c21bdbd39b6d66643bcc59ee5534cc2f00\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "MvkwDJOfd07F"
      },
      "id": "MvkwDJOfd07F",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ea22d710-40f5-4b64-803e-83c583aa3472",
      "metadata": {
        "id": "ea22d710-40f5-4b64-803e-83c583aa3472"
      },
      "outputs": [],
      "source": [
        "# First Load all the required library and also Start Spark Session\n",
        "# Load all the required library\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "368cc9c2-6f26-4d6b-93ff-d9ee7541869c",
      "metadata": {
        "id": "368cc9c2-6f26-4d6b-93ff-d9ee7541869c"
      },
      "outputs": [],
      "source": [
        "#Start Spark Session\n",
        "spark = SparkSession.builder.appName(\"chapter0\").getOrCreate()\n",
        "sqlContext = SparkSession(spark)\n",
        "#Dont Show warning only error\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aad8fd73-3ae0-4bfc-b429-bbb7b34ec39e",
      "metadata": {
        "id": "aad8fd73-3ae0-4bfc-b429-bbb7b34ec39e"
      },
      "outputs": [],
      "source": [
        "#Load CSV file into DataFrame\n",
        "csvdf = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"nyc_taxi_zone.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "af0fcbe3-02b6-41b2-8b0f-bc6fbf8ad371",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af0fcbe3-02b6-41b2-8b0f-bc6fbf8ad371",
        "outputId": "c0309b07-efa5-44a9-fba3-a39c94eede21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- LocationID: integer (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Checking dataframe schema\n",
        "csvdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0e2fb0de-d2a6-4cd1-8e8d-c3a483e59e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e2fb0de-d2a6-4cd1-8e8d-c3a483e59e80",
        "outputId": "f2e586ff-eda8-4ab3-d927-5948ee3be84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+--------------------+------------+\n",
            "|LocationID|      Borough|                Zone|service_zone|\n",
            "+----------+-------------+--------------------+------------+\n",
            "|         1|          EWR|      Newark Airport|         EWR|\n",
            "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
            "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
            "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
            "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
            "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
            "|         7|       Queens|             Astoria|   Boro Zone|\n",
            "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
            "|         9|       Queens|          Auburndale|   Boro Zone|\n",
            "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
            "+----------+-------------+--------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "csvdf.show(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b6425f69-882b-4604-9845-cda68e1030d1",
      "metadata": {
        "id": "b6425f69-882b-4604-9845-cda68e1030d1"
      },
      "outputs": [],
      "source": [
        "#Load Json file into DataFrame\n",
        "jsondf = spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"nyc_taxi_zone.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "939bd0b2-7335-4dda-991e-670675df8edb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "939bd0b2-7335-4dda-991e-670675df8edb",
        "outputId": "51206d7d-2c8f-4672-97c9-e120deebb0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- LocationID: long (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "jsondf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "87ea9992-257c-49f7-974c-9ad64a6695f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ea9992-257c-49f7-974c-9ad64a6695f5",
        "outputId": "1526e2bd-901a-434f-a4b3-5b0388a31f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+--------------------+------------+\n",
            "|      Borough|LocationID|                Zone|service_zone|\n",
            "+-------------+----------+--------------------+------------+\n",
            "|          EWR|         1|      Newark Airport|         EWR|\n",
            "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
            "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
            "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
            "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
            "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
            "|       Queens|         7|             Astoria|   Boro Zone|\n",
            "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
            "|       Queens|         9|          Auburndale|   Boro Zone|\n",
            "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
            "+-------------+----------+--------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "jsondf.show(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d856527c-c169-4132-b9bc-3dc59e5bab66",
      "metadata": {
        "id": "d856527c-c169-4132-b9bc-3dc59e5bab66"
      },
      "outputs": [],
      "source": [
        "parquetdf = spark.read.format(\"parquet\").load(\"yellow_tripdata_2022-01.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 yellow_tripdata_2022-01.parquet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czh20eV6e2su",
        "outputId": "e04d0946-64fa-4976-e401-9809f9998e31"
      },
      "id": "Czh20eV6e2su",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PAR1\u0015\u0004\u0015@\u0015>L\u0015\b\u0015\u0004\u0012\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\n",
            "cd�\u0000&(�\u0006�Y�4\u0000���* \u0000\u0000\u0000\u0015\u0000\u0015��I\u0015��&,\u0015��\u0002\u0015\u0004\u0015\u0006\u0015\u0006\u001c\u0018\b\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000(\b\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\n",
            "L�ݒ%G�������������0���p\u0004\u0002�@$\u0012\u0019�D�QhTVv�Zα�Ꚛ�\u001aE�e8\u0014�P(�\u001c�\u0005��P|*>\u0004����Od��2\u0013�p?�L���k�]RJ���������U�����֏u�W�}�7i\u001c�h�x��|\u001c�9/�\u001e��aJ�Ƚ�ӑ�~�)_����zȯZm��:j��q\u001c��O��\u001fU��<��\u000e}~?\u001e�%��q<��Ԏ\\�~~;r;>�z�/z��]t$�])c�7��GZ��#�\u0007�9��m�C����I�r:ʑ�1�]Ǿ\u001f����R/I�\u0019������g>�%_��(C\u000f����c�\u001b��\u001a��/)���=���ul�!��z��AϠ��\u001f|�>|=JKǳ\u001e~�����yh��\u0005u������E?�W�\n",
            "\u001e����c�\b\u000b/3��r\u001c�]��9�M\u000b�Om�'�ˑ�~!g�����H��#���^�\u0017�aM�\u0007-ja��67�u���\u001f��o{��j!�>D���w�j�w<�Yy\u001ccϽ�:����>��\u001f؍c���\bɯ�5z\b~V�=��?�k��>�%�M��K����Jj�C.�/��)U�]I#���nsl�����b��9\u000f��mH�'}��\u0005o�\u0012����U��O����,�v��>�-�?j�\u0007�z�u�1\u001a���{\u001dH\u001d8-�6�֤}��~m\u001b|����ϮǱ곫>��e�\u000e\u001f�}�\u0011ז�����c+��il�Hy�\u00119�R'-��<\u0014\u000f��N�p\u001dsݍ�k�t�tX޳\u0001���;�h㧴��]Ig�\u0016\u000e�^��Z���\u000e\u000e\bO�\u000fΦ��'6�\t�[�\r�.(\u0007q����}ӗ�r�I\u001fv�Rʩ|do�.\u001d_^?����h�X-�I?��M\u0017�>iC+�������)̉���\n",
            "?��}^��Z^~Go��9y�����O:\u0000,g�����\u0003�s���.���լSqӸ\u0014[;�t�è'J�0\u001b7�aL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7de65113-d647-49db-a7a3-6b8e6b1145ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7de65113-d647-49db-a7a3-6b8e6b1145ab",
        "outputId": "83102edf-4986-40b3-d711-379b75a53d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- VendorID: long (nullable = true)\n",
            " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
            " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
            " |-- passenger_count: double (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: double (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: long (nullable = true)\n",
            " |-- DOLocationID: long (nullable = true)\n",
            " |-- payment_type: long (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- congestion_surcharge: double (nullable = true)\n",
            " |-- airport_fee: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parquetdf.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d6f6fd8e-6367-494f-9682-a901d2822473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6f6fd8e-6367-494f-9682-a901d2822473",
        "outputId": "6ab2dfcd-e65e-426e-dd9e-928cb7bd39fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2463931"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "parquetdf.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "271feb0b-02c7-4f12-870a-97abe0c0a80c",
      "metadata": {
        "id": "271feb0b-02c7-4f12-870a-97abe0c0a80c"
      },
      "outputs": [],
      "source": [
        "txtdf = spark.read.text(\"sample.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "658d4a24-9c2a-489c-b167-1a4dfe2064de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "658d4a24-9c2a-489c-b167-1a4dfe2064de",
        "outputId": "1269fda0-dcbd-4505-a0c4-d871027a3b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|      value|\n",
            "+-----------+\n",
            "|1,chris,USA|\n",
            "| 2,Mark,AUS|\n",
            "| 3,Jags,IND|\n",
            "|  4,Adam,UK|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "txtdf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d84a2e3e-62e1-4521-8888-e2108dcbcd73",
      "metadata": {
        "id": "d84a2e3e-62e1-4521-8888-e2108dcbcd73"
      },
      "outputs": [],
      "source": [
        "txtdf = spark.read.option(\"lineSep\", \",\").text(\"sample.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d7a06580-48ce-4b15-8cf8-a784b7c6d9e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7a06580-48ce-4b15-8cf8-a784b7c6d9e7",
        "outputId": "1fe270b7-a333-44e8-b5e1-93640b48ee4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|     1|\n",
            "| chris|\n",
            "|USA\\n2|\n",
            "|  Mark|\n",
            "|AUS\\n3|\n",
            "|  Jags|\n",
            "|IND\\n4|\n",
            "|  Adam|\n",
            "|    UK|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "txtdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db50d782-2616-436a-9efb-103b6c6a30ca",
      "metadata": {
        "id": "db50d782-2616-436a-9efb-103b6c6a30ca"
      },
      "source": [
        "Create temp table for all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dc044f1f-10e4-469a-9b6b-cce323543784",
      "metadata": {
        "id": "dc044f1f-10e4-469a-9b6b-cce323543784"
      },
      "outputs": [],
      "source": [
        "csvdf.createOrReplaceTempView(\"tempCSV\")\n",
        "jsondf.createOrReplaceTempView(\"tempJSON\")\n",
        "parquetdf.createOrReplaceTempView(\"tempParquet\")\n",
        "txtdf.createOrReplaceTempView(\"tempTXT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7922b05f-45f4-4485-8f67-06ce893175fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7922b05f-45f4-4485-8f67-06ce893175fa",
        "outputId": "03ca42e2-dfb1-4079-ea52-511759c6431b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+--------------------+------------+\n",
            "|LocationID|      Borough|                Zone|service_zone|\n",
            "+----------+-------------+--------------------+------------+\n",
            "|         1|          EWR|      Newark Airport|         EWR|\n",
            "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
            "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
            "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
            "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
            "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
            "|         7|       Queens|             Astoria|   Boro Zone|\n",
            "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
            "|         9|       Queens|          Auburndale|   Boro Zone|\n",
            "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
            "+----------+-------------+--------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"SELECT * FROM tempCSV LIMIT 10\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1047953f-cbdd-41d6-9dd6-b78cd6cbff9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1047953f-cbdd-41d6-9dd6-b78cd6cbff9a",
        "outputId": "bca178c3-34ed-4e51-e6d8-74e3fa9c2803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+--------------------+------------+\n",
            "|      Borough|LocationID|                Zone|service_zone|\n",
            "+-------------+----------+--------------------+------------+\n",
            "|          EWR|         1|      Newark Airport|         EWR|\n",
            "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
            "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
            "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
            "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
            "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
            "|       Queens|         7|             Astoria|   Boro Zone|\n",
            "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
            "|       Queens|         9|          Auburndale|   Boro Zone|\n",
            "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
            "+-------------+----------+--------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"SELECT * FROM tempJSON LIMIT 10\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "10a62472-1c28-487c-9d34-40de77fdac2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10a62472-1c28-487c-9d34-40de77fdac2c",
        "outputId": "2015d0cd-1475-4c46-838a-ea30cd4a6d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|  count|\n",
            "+-------+\n",
            "|2463931|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"SELECT count(*) as count FROM tempParquet\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac1b4fa-f17f-4cb2-94a6-bdbb0e63a5c0",
      "metadata": {
        "id": "1ac1b4fa-f17f-4cb2-94a6-bdbb0e63a5c0"
      },
      "source": [
        " Create JSON file from CSV dataframe\n",
        "\n",
        "Save Options\n",
        "- append\n",
        "- overwrite\n",
        "- ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fcf9438b-9165-4b09-ac20-f3e722ccc2ed",
      "metadata": {
        "id": "fcf9438b-9165-4b09-ac20-f3e722ccc2ed"
      },
      "outputs": [],
      "source": [
        "csvdf.write.format(\"json\").save(\"jsondata\",mode='append')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154a8531-1862-446c-b57e-3cd88f5046a6",
      "metadata": {
        "id": "154a8531-1862-446c-b57e-3cd88f5046a6"
      },
      "source": [
        " Create CSV file from Parquet dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1eb72982-7e1e-4b9b-96e2-f0e3eb8f34ff",
      "metadata": {
        "id": "1eb72982-7e1e-4b9b-96e2-f0e3eb8f34ff"
      },
      "outputs": [],
      "source": [
        "parquetdf.write.format(\"csv\").option(\"header\",\"true\").save(\"csvdata\",mode='append')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7aab30-01ba-4c78-b4d9-7e30ce50e5c8",
      "metadata": {
        "id": "cc7aab30-01ba-4c78-b4d9-7e30ce50e5c8"
      },
      "source": [
        " Create parquet file from JSON dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a21f9550-c247-44bf-84e6-49967512615b",
      "metadata": {
        "id": "a21f9550-c247-44bf-84e6-49967512615b"
      },
      "outputs": [],
      "source": [
        "jsondf.write.format(\"parquet\").option(\"compression\",\"snappy\").save(\"parquetdata\",mode='append')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create orc file from csv dataframe"
      ],
      "metadata": {
        "id": "0sHlT29s3aRV"
      },
      "id": "0sHlT29s3aRV"
    },
    {
      "cell_type": "code",
      "source": [
        "csvdf.write.format(\"orc\").save(\"orcdata\",mode='append')"
      ],
      "metadata": {
        "id": "VqJS3aWt04Wa"
      },
      "id": "VqJS3aWt04Wa",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create avro file from csv dataframe"
      ],
      "metadata": {
        "id": "4B4K71T1yIQv"
      },
      "id": "4B4K71T1yIQv"
    },
    {
      "cell_type": "code",
      "source": [
        "csvdf.write.format(\"avro\").save(\"avrodata\",mode='append')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "mN6E4lPNxIsU",
        "outputId": "2f8cb62d-58a6-4ab4-fde6-aede1983a8db"
      },
      "id": "mN6E4lPNxIsU",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ee06ef84fee5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsvdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avrodata\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install avro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az3sp2wU_bIs",
        "outputId": "c6ce1d46-dd00-4272-c1cb-b9b9fd071c03"
      },
      "id": "Az3sp2wU_bIs",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting avro\n",
            "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: avro\n",
            "  Building wheel for avro (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123913 sha256=33609bb9cfca9e1d80ea253bd9d12febe2b719b239f87994984113c0004a6429\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/f6/41/0e0399396af07060e64d4e32c8bd259b48b98a4a114df31294\n",
            "Successfully built avro\n",
            "Installing collected packages: avro\n",
            "Successfully installed avro-1.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import avro"
      ],
      "metadata": {
        "id": "l-4SzWhp_jzr"
      },
      "id": "l-4SzWhp_jzr",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark-avro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppCCDgwY-z6A",
        "outputId": "4b82c543-eac3-4aba-d102-4dc70f36fa65"
      },
      "id": "ppCCDgwY-z6A",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pyspark-avro (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pyspark-avro\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.avro"
      ],
      "metadata": {
        "id": "zHU7xnd2_81i"
      },
      "id": "zHU7xnd2_81i",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-avro_2.12:3.2.0 chapter0.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8laNoXA0nt",
        "outputId": "183061d4-1224-4e88-e691-cc906ee7799e"
      },
      "id": "6f8laNoXA0nt",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-avro_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c90397cf-860f-4fb5-8012-3d80015dfce9;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
            "\tfound org.tukaani#xz;1.8 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.0/spark-avro_2.12-3.2.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.2.0!spark-avro_2.12.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.8/xz-1.8.jar ...\n",
            "\t[SUCCESSFUL ] org.tukaani#xz;1.8!xz.jar (18ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (10ms)\n",
            ":: resolution report :: resolve 1741ms :: artifacts dl 73ms\n",
            "\t:: modules in use:\n",
            "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.tukaani#xz;1.8 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c90397cf-860f-4fb5-8012-3d80015dfce9\n",
            "\tconfs: [default]\n",
            "\t3 artifacts copied, 0 already retrieved (290kB/12ms)\n",
            "24/02/03 09:47:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "python3: can't open file '/content/chapter0.py': [Errno 2] No such file or directory\n",
            "24/02/03 09:47:40 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/02/03 09:47:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-f9833a93-e0f5-424e-8d24-bee3dd2d0aab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csvdf.write.format(\"avro\").mode('append').save(\"avrodata\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "CsoQrfL2YkuE",
        "outputId": "1071a1bc-3231-4de7-b59a-b58d553a1cf0"
      },
      "id": "CsoQrfL2YkuE",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9bc4fbcf2d6a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsvdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avrodata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}